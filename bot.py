import os
import json
import re
import time
import logging
import asyncio
import traceback
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, unquote, urlunparse, quote

import aiohttp
import requests
from bs4 import BeautifulSoup
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    filters,
    ContextTypes,
)

# ----------------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† -----------------
class Storage:
    """ØªØ®Ø²ÙŠÙ† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†"""
    def __init__(self):
        self.user_sessions = {}
        self.processing_users = set()
        
    def is_processing(self, user_id: int) -> bool:
        return user_id in self.processing_users
        
    def set_processing(self, user_id: int, status: bool):
        if status:
            self.processing_users.add(user_id)
        else:
            self.processing_users.discard(user_id)
            
    def get_session(self, user_id: int) -> dict:
        if user_id not in self.user_sessions:
            self.user_sessions[user_id] = {
                'last_url': None,
                'last_title': None,
                'episode_number': None,
                'auto_mode': False,
                'history': []
            }
        return self.user_sessions[user_id]

storage = Storage()

# ----------------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) -----------------
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ----------------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª -----------------
TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN", "YOUR_BOT_TOKEN_HERE")
ADMIN_IDS = json.loads(os.environ.get("ADMIN_IDS", "[]"))
MAX_EPISODES_PER_RUN = 50
REQUEST_TIMEOUT = 30
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

# ----------------- Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© -----------------
def extract_base_url(url: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
    parsed_url = urlparse(url)
    return f"{parsed_url.scheme}://{parsed_url.netloc}"

def extract_title_from_url(url: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    parsed_url = urlparse(url)
    path = unquote(parsed_url.path)
    path_parts = path.strip('/').split('-')
    title = ' '.join(path_parts).replace('.html', '').title()
    
    if title.startswith("Ù…Ø³Ù„Ø³Ù„"):
        words = title.split()
        new_title = []
        for word in words:
            new_title.append(word)
            if any(char.isdigit() for char in word):
                break
        title = ' '.join(new_title)
    
    return title

async def follow_redirect(url: str, session: aiohttp.ClientSession, max_redirects: int = 5) -> Optional[str]:
    """ØªØªØ¨Ø¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡"""
    redirect_count = 0
    current_url = url
    
    while redirect_count < max_redirects:
        try:
            async with session.get(current_url, allow_redirects=False, timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)) as response:
                if response.status in (301, 302, 303, 307, 308) and 'location' in response.headers:
                    redirect_count += 1
                    current_url = response.headers['location']
                    if not current_url.startswith(('http://', 'https://')):
                        base = extract_base_url(url)
                        current_url = base + current_url
                else:
                    return str(response.url)
        except Exception as e:
            logger.error(f"Error following redirect: {e}")
            return None
    
    return current_url

def find_last_numeric_segment_in_path(path_unquoted: str) -> Tuple[Optional[int], Optional[str]]:
    """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø£Ø®ÙŠØ± ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±"""
    parts = path_unquoted.strip('/').split('-')
    for i in range(len(parts)-1, -1, -1):
        if re.fullmatch(r'\d+', parts[i]):
            return i, parts[i]
    return None, None

def build_episode_url_from_any(url: str, episode_number: int) -> Optional[str]:
    """Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø©"""
    p = urlparse(url)
    path_unquoted = unquote(p.path)
    idx, num = find_last_numeric_segment_in_path(path_unquoted)
    
    if idx is None:
        return None
    
    parts = path_unquoted.strip('/').split('-')[:idx+1]
    parts[-1] = str(episode_number)
    new_path = '/' + '-'.join(parts)
    quoted_path = quote(new_path, safe="/%")
    new_parsed = (p.scheme, p.netloc, quoted_path, '', '', '')
    return urlunparse(new_parsed)

def extract_episode_and_base(url: str) -> Tuple[Optional[int], Optional[callable]]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø© ÙˆØ¯Ø§Ù„Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡"""
    p = urlparse(url)
    path_unquoted = unquote(p.path)
    idx, num = find_last_numeric_segment_in_path(path_unquoted)
    
    if idx is None or num is None:
        return None, None
    
    return int(num), lambda ep: build_episode_url_from_any(url, ep)

# ----------------- Ø¯ÙˆØ§Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© -----------------
async def get_download_info(server_href: str, referer: str) -> Optional[Dict]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø±Ø§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±"""
    try:
        timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)
        
        async with aiohttp.ClientSession(
            headers={"User-Agent": USER_AGENT, "Referer": referer},
            timeout=timeout
        ) as session:
            
            # ØªØªØ¨Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡
            redirected = await follow_redirect(server_href, session)
            if not redirected:
                return None
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· ?r=
            r_link = None
            if '?r=' in redirected:
                r_link = redirected
            else:
                async with session.get(redirected) as response:
                    text = await response.text()
                    match = re.search(r'(https?://[^"\'>\s]+/category/downloadz/\?r=\d+[^"\'>\s]*)', text)
                    if match:
                        r_link = match.group(1)
                    elif '?r=' in str(response.url):
                        r_link = str(response.url)
            
            if not r_link:
                return None
            
            # ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„
            async with session.get(r_link) as response:
                text = await response.text()
                soup = BeautifulSoup(text, 'html.parser')
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
                btn_tag = soup.find('a', id='btn') or soup.select_one('a.downloadbtn')
                final_asd_url = None
                
                if btn_tag and btn_tag.get('href'):
                    candidate = btn_tag.get('href')
                    if candidate.startswith('/'):
                        candidate = extract_base_url(r_link) + candidate
                    final_asd_url = candidate
                else:
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹
                    dynamic_param_pattern = r'([?&][a-zA-Z0-9_]+\d*=[^"&\']+)'
                    qs_matches = re.findall(dynamic_param_pattern, text)
                    params = []
                    for q in qs_matches:
                        normalized_param = q.lstrip('?&')
                        if normalized_param.lower().startswith('r='):
                            continue
                        param_name = normalized_param.split('=', 1)[0]
                        if not any(p.startswith(param_name + '=') for p in params):
                            params.append(normalized_param)
                    
                    if params:
                        sep = '&' if '?' in r_link else '?'
                        final_asd_url = r_link + sep + '&'.join(params)
                
                if not final_asd_url:
                    final_asd_url = r_link
                
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
                async with session.get(final_asd_url) as final_resp:
                    final_text = await final_resp.text()
                    final_soup = BeautifulSoup(final_text, 'html.parser')
                    
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· MP4
                    final_tag = (final_soup.find('a', id='btn') or 
                                final_soup.find('a', class_='downloadbtn') or 
                                final_soup.find('a', href=re.compile(r'\.mp4')))
                    
                    if not final_tag:
                        return None
                    
                    file_link = final_tag.get('href')
                    if file_link and file_link.startswith('/'):
                        file_link = extract_base_url(final_asd_url) + file_link
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù ÙˆØ­Ø¬Ù…Ù‡
                    file_name = None
                    file_size = None
                    
                    name_span = final_soup.select_one('.TitleCenteral h3 span')
                    if name_span:
                        file_name = name_span.get_text(strip=True)
                    
                    size_span = final_soup.select_one('.TitleCenteral h3:nth-of-type(2) span')
                    if size_span:
                        file_size = size_span.get_text(strip=True)
                    
                    if not file_size:
                        h3 = final_soup.find('h3')
                        if h3:
                            msize = re.search(r'Ø§Ù„Ø­Ø¬Ù…[:\s\-â€“]*([\d\.,]+\s*(?:MB|GB))', h3.get_text())
                            if msize:
                                file_size = msize.group(1)
                    
                    if not file_name:
                        file_name = os.path.basename(file_link) if file_link else "unknown"
                    
                    return {
                        'direct_link': file_link.replace(" ", ".") if file_link else None,
                        'file_name': file_name,
                        'file_size': file_size or "Unknown"
                    }
    
    except Exception as e:
        logger.error(f"Error in get_download_info: {e}")
        return None

async def process_arabseed_url(url: str, session: aiohttp.ClientSession) -> Tuple[bool, str, List[List[InlineKeyboardButton]]]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø§Ø¨Ø· Ø¹Ø±Ø¨ Ø³ÙŠØ¯"""
    try:
        # ÙØ­Øµ Ø§Ù„Ø±Ø§Ø¨Ø·
        if not url.startswith(('http://', 'https://')):
            return False, "âŒ Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­!", []
        
        # ØªØªØ¨Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø®ØªØµØ±Ø©
        if '/l/' in url or 'reviewrate.net' in url:
            url = await follow_redirect(url, session) or url
        
        async with session.get(url, timeout=REQUEST_TIMEOUT) as response:
            if response.status != 200:
                return False, f"âŒ Ø§Ù„Ø±Ø§Ø¨Ø· ØºÙŠØ± Ù…ØªØ§Ø­ (Ø±Ù…Ø²: {response.status})", []
            
            text = await response.text()
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„ØµÙØ­Ø©
            if any(phrase in text.lower() for phrase in ['Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ±', 'page not found', 'ØµÙØ­Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©']):
                return False, "âŒ Ø§Ù„Ø­Ù„Ù‚Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©!", []
            
            soup = BeautifulSoup(text, 'html.parser')
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· ØµÙØ­Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„
            download_anchor = soup.find('a', href=re.compile(r'/download/')) or soup.find('a', class_=re.compile(r'download__btn|downloadBTn'))
            if not download_anchor:
                return False, "âŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„!", []
            
            quality_page_url = download_anchor.get('href')
            if quality_page_url.startswith('/'):
                quality_page_url = extract_base_url(url) + quality_page_url
            
            # Ø²ÙŠØ§Ø±Ø© ØµÙØ­Ø© Ø§Ù„Ø¬ÙˆØ¯Ø§Øª
            async with session.get(quality_page_url, headers={'Referer': extract_base_url(url)}) as qresp:
                if qresp.status != 200:
                    return False, "âŒ ØµÙØ­Ø© Ø§Ù„Ø¬ÙˆØ¯Ø§Øª ØºÙŠØ± Ù…ØªØ§Ø­Ø©!", []
                
                qtext = await qresp.text()
                qsoup = BeautifulSoup(qtext, 'html.parser')
                
                # Ø¬Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª
                server_links = qsoup.find_all('a', href=re.compile(r'/l/'))
                if not server_links:
                    server_links = qsoup.select('ul.downloads__links__list a')
                
                if not server_links:
                    return False, "âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· ØªØ­Ù…ÙŠÙ„ Ù…ØªØ§Ø­Ø©!", []
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø³ÙŠØ±ÙØ±
                buttons = []
                seen_qualities = set()
                
                for a in server_links:
                    href = a.get('href')
                    if not href:
                        continue
                    
                    # ØªØ®Ø·ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ØºÙŠØ± Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
                    if 'arabseed' not in href and 'Ø¹Ø±Ø¨ Ø³ÙŠØ¯' not in a.get_text(" ", strip=True):
                        continue
                    
                    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙˆØ¯Ø©
                    quality = "Unknown"
                    parent_with_quality = a.find_parent(attrs={"data-quality": True})
                    if parent_with_quality:
                        quality = parent_with_quality.get('data-quality')
                    else:
                        ptxt = a.get_text(" ", strip=True)
                        qmatch = re.search(r'(\d{3,4}p)', ptxt)
                        if qmatch:
                            quality = qmatch.group(1)
                    
                    if quality in seen_qualities:
                        continue
                    seen_qualities.add(quality)
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„
                    info = await get_download_info(href, extract_base_url(quality_page_url))
                    if info and info.get('direct_link'):
                        btn_text = f"ğŸ“¥ {quality} ({info.get('file_size', '?')})"
                        buttons.append([InlineKeyboardButton(btn_text, url=info['direct_link'])])
                
                if not buttons:
                    return False, "âŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„!", []
                
                title = extract_title_from_url(url)
                return True, title, buttons
    
    except asyncio.TimeoutError:
        return False, "â° Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù„Ù„Ø·Ù„Ø¨!", []
    except Exception as e:
        logger.error(f"Error processing URL: {e}\n{traceback.format_exc()}")
        return False, f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}", []

# ----------------- Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Telegram -----------------
async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ù…Ø± /start"""
    user = update.effective_user
    
    welcome_text = f"""
ğŸ¬ Ù…Ø±Ø­Ø¨Ø§Ù‹ {user.first_name}!
    
ğŸ¤– Ø£Ù†Ø§ Ø¨ÙˆØª Ù„ØªØ­Ù…ÙŠÙ„ Ø­Ù„Ù‚Ø§Øª Ø¹Ø±Ø¨ Ø³ÙŠØ¯ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ ØªÙ„ÙŠØ¬Ø±Ø§Ù….

ğŸ“Œ *Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:*
1. Ø£Ø±Ø³Ù„ Ù„ÙŠ Ø±Ø§Ø¨Ø· Ø­Ù„Ù‚Ø© Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø¹Ø±Ø¨ Ø³ÙŠØ¯
2. Ø³Ø£Ø±Ø³Ù„ Ù„Ùƒ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©

âš¡ *Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¨ÙˆØª:*
â€¢ ØªØ­Ù…ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¨Ø¬ÙˆØ¯Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
â€¢ Ø¯Ø¹Ù… Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø®ØªØµØ±Ø©
â€¢ ÙˆØ§Ø¬Ù‡Ø© Ø³Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
â€¢ ÙŠØ¹Ù…Ù„ 24/7

ğŸ”— *Ù…Ø«Ø§Ù„ Ù„Ù„Ø±Ø§Ø¨Ø·:*
https://arabseed.cam/Ù…Ø³Ù„Ø³Ù„-Ø§Ù„Ø¹Ù†ÙƒØ¨ÙˆØª-Ø§Ù„Ø­Ù„Ù‚Ø©-1.html

ğŸ“¢ *Ù‚Ù†Ø§Ø© Ø§Ù„Ø¨ÙˆØª:* @ArabSeed_DL_Bot
    """
    
    keyboard = [
        [InlineKeyboardButton("ğŸ¬ Ø§Ø±Ø³Ø§Ù„ Ø±Ø§Ø¨Ø·", switch_inline_query_current_chat="")],
        [InlineKeyboardButton("ğŸ“¢ Ù‚Ù†Ø§Ø© Ø§Ù„Ø¨ÙˆØª", url="https://t.me/ArabSeed_DL_Bot")]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_text(welcome_text, reply_markup=reply_markup, parse_mode='Markdown')

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ù…Ø± /help"""
    help_text = """
ğŸ“– *Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙˆØª:*

ğŸ”— *ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:*
1. Ù‚Ù… Ø¨Ù†Ø³Ø® Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø© Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø¹Ø±Ø¨ Ø³ÙŠØ¯
2. Ø£Ø±Ø³Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ù†Ø§ ÙÙŠ Ø§Ù„Ø¨ÙˆØª
3. Ø§Ù†ØªØ¸Ø± Ø­ØªÙ‰ Ø£Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø·
4. Ø³Ø£Ø±Ø³Ù„ Ù„Ùƒ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©

âš ï¸ *Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù‡Ø§Ù…Ø©:*
â€¢ Ø§Ù„Ø¨ÙˆØª ÙŠØ¯Ø¹Ù… Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙ‚Ø·
â€¢ Ù‚Ø¯ Ù„Ø§ ØªØ¹Ù…Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
â€¢ Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ØµÙ„ÙŠ
â€¢ Ø§Ù„Ø¨ÙˆØª Ù„Ø§ ÙŠØ®Ø²Ù† Ø£ÙŠ Ù…Ù„ÙØ§Øª Ø¹Ù„Ù‰ Ø³ÙŠØ±ÙØ±Ø§ØªÙ‡

ğŸ“ *Ù„Ù„ØªÙˆØ§ØµÙ„ ÙˆØ§Ù„Ø¯Ø¹Ù…:* @ArabSeed_Support
    """
    
    await update.message.reply_text(help_text, parse_mode='Markdown')

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù†ØµÙŠØ©"""
    user_id = update.effective_user.id
    message = update.message
    
    if storage.is_processing(user_id):
        await message.reply_text("â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ Ø§Ù„Ø³Ø§Ø¨Ù‚ØŒ Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹...")
        return
    
    url = message.text.strip()
    
    if not url.startswith(('http://', 'https://')):
        await message.reply_text("âŒ Ù‡Ø°Ø§ Ù„ÙŠØ³ Ø±Ø§Ø¨Ø·Ø§Ù‹ ØµØ§Ù„Ø­Ø§Ù‹!")
        return
    
    storage.set_processing(user_id, True)
    
    try:
        # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
        wait_msg = await message.reply_text("â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø§Ø¨Ø·ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±...")
        
        async with aiohttp.ClientSession(headers={"User-Agent": USER_AGENT}) as session:
            success, title, buttons = await process_arabseed_url(url, session)
        
        if success:
            response_text = f"""
ğŸ¬ *{title}*

ğŸ“¥ *Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØ§Ø­Ø©:*
Ø§Ø®ØªØ± Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø£Ø¯Ù†Ø§Ù‡.

ğŸ”” *Ù…Ù„Ø§Ø­Ø¸Ø©:* Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø³ÙŠØ±ÙØ±Ø§Øª Ø¹Ø±Ø¨ Ø³ÙŠØ¯
            """
            
            keyboard = InlineKeyboardMarkup(buttons + [
                [InlineKeyboardButton("ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø§Ø¨Ø· Ø¢Ø®Ø±", callback_data="new_link")],
                [InlineKeyboardButton("ğŸ“¢ Ù‚Ù†Ø§Ø© Ø§Ù„Ø¨ÙˆØª", url="https://t.me/ArabSeed_DL_Bot")]
            ])
            
            await wait_msg.delete()
            await message.reply_text(response_text, reply_markup=keyboard, parse_mode='Markdown')
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
            storage.get_session(user_id)['history'].append({
                'url': url,
                'title': title,
                'time': datetime.now().isoformat()
            })
        else:
            await wait_msg.delete()
            await message.reply_text(f"{title}\n\nâš ï¸ ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
    
    except Exception as e:
        logger.error(f"Error in handle_message: {e}")
        await message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©ØŒ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
    
    finally:
        storage.set_processing(user_id, False)

async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¶ØºØ·Ø§Øª Ø§Ù„Ø£Ø²Ø±Ø§Ø±"""
    query = update.callback_query
    await query.answer()
    
    if query.data == "new_link":
        await query.edit_message_text("ğŸ”„ Ø£Ø±Ø³Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©...")

async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨ÙˆØª (Ù„Ù„Ù…Ø´Ø±ÙÙŠÙ† ÙÙ‚Ø·)"""
    user_id = update.effective_user.id
    
    if user_id not in ADMIN_IDS:
        await update.message.reply_text("âŒ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± Ù„Ù„Ù…Ø´Ø±ÙÙŠÙ† ÙÙ‚Ø·!")
        return
    
    stats_text = f"""
ğŸ“Š *Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¨ÙˆØª:*

ğŸ‘¥ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ù†Ø´Ø·ÙŠÙ†: {len(storage.user_sessions)}
ğŸ”„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ù‚ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {len(storage.processing_users)}
â° ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„: {time.strftime('%H:%M:%S', time.gmtime(time.time() - start_time))}

ğŸ“… Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    """
    
    await update.message.reply_text(stats_text, parse_mode='Markdown')

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    logger.error(f"Update {update} caused error {context.error}")
    
    if update and update.effective_message:
        await update.effective_message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")

# ----------------- Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ -----------------
def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª"""
    global start_time
    start_time = time.time()
    
    print("ğŸ¬ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø¨ÙˆØª Ø¹Ø±Ø¨ Ø³ÙŠØ¯...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    application = Application.builder().token(TOKEN).build()
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("stats", stats_command))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    application.add_handler(CallbackQueryHandler(button_callback))
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    application.add_error_handler(error_handler)
    
    # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
    print("ğŸ¤– Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù†! Ø§Ø¶ØºØ· Ctrl+C Ù„Ø¥ÙŠÙ‚Ø§ÙÙ‡.")
    
    # Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ…Ø±
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()
